{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents import SearchClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import uuid  # Import the uuid module\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "service_endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]\n",
    "index_name = os.environ[\"AZURE_SEARCH_INDEX_NAME\"]\n",
    "key = os.environ[\"AZURE_SEARCH_API_KEY\"]\n",
    "\n",
    "# Create a SearchClient\n",
    "search_client = SearchClient(service_endpoint, index_name, AzureKeyCredential(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "def upload_bulk_documents(documents):\n",
    "    \"\"\"\n",
    "    Uploads a batch of documents to the Azure Cognitive Search index.\n",
    "    Each document in the batch will have a base UUID, and each paragraph will have a unique ID derived from it.\n",
    "    \n",
    "    :param documents: A list of dictionaries, where each dictionary represents a document paragraph.\n",
    "                      Each dictionary must contain the keys: \"base_id\", \"document_name\", \"document_type\",\n",
    "                      \"document_link\", \"issuer\", \"resource_name\", and \"content\".\n",
    "    \"\"\"\n",
    "    # Prepare the documents for upload\n",
    "    upload_documents = []\n",
    "    paragraph_counts = {}  # Track paragraph numbering for each base_id\n",
    "\n",
    "    for doc in documents:\n",
    "        base_id = doc[\"base_id\"]\n",
    "\n",
    "        # Increment paragraph count for this document\n",
    "        if base_id not in paragraph_counts:\n",
    "            paragraph_counts[base_id] = 1\n",
    "        else:\n",
    "            paragraph_counts[base_id] += 1\n",
    "\n",
    "        # Create a unique document ID by appending a paragraph number\n",
    "        unique_doc_id = f\"{base_id}_{paragraph_counts[base_id]}\"\n",
    "\n",
    "        # Create the document dictionary\n",
    "        document = {\n",
    "            \"id\": unique_doc_id,  # Unique ID per paragraph\n",
    "            \"document_name\": doc[\"document_name\"],\n",
    "            \"document_type\": doc[\"document_type\"],\n",
    "            \"document_link\": doc[\"document_link\"],\n",
    "            \"issuer\": doc[\"issuer\"],\n",
    "            \"resource_name\": doc[\"resource_name\"],\n",
    "            \"content\": doc[\"content\"]\n",
    "        }\n",
    "\n",
    "        # Add the document to the upload list\n",
    "        upload_documents.append(document)\n",
    "\n",
    "    # Print the prepared documents before uploading\n",
    "    for document in upload_documents:\n",
    "        print(document)\n",
    "\n",
    "    # Upload the batch of documents\n",
    "    search_client.upload_documents(documents=upload_documents)\n",
    "    print(f\"Prepared {len(upload_documents)} documents for upload.\")\n",
    "\n",
    "# Example usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "def load_documents_from_json(directory):\n",
    "    documents_to_upload = []\n",
    "    document_uuid_map = {}  # Store base UUIDs for each document\n",
    "\n",
    "    # Get all JSON files from the directory\n",
    "    json_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".json\")]\n",
    "\n",
    "    for file in json_files:\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        documents = data.get(\"documents\", [])\n",
    "\n",
    "        for doc in documents:\n",
    "            document_name = doc.get(\"document_name\", \"Unknown Document\")\n",
    "\n",
    "            # Generate or retrieve a unique base UUID for this document\n",
    "            if document_name not in document_uuid_map:\n",
    "                document_uuid_map[document_name] = str(uuid.uuid4())\n",
    "\n",
    "            base_id = document_uuid_map[document_name]\n",
    "            document_type = doc.get(\"document_type\", \"Unknown Type\")\n",
    "            document_link = doc.get(\"document_link\", \"\")\n",
    "            issuer = doc.get(\"issuer\", \"Unknown Issuer\")\n",
    "            resource_name = doc.get(\"resource_name\", \"Unknown Resource\")\n",
    "            content_dict = doc.get(\"content\", {})\n",
    "\n",
    "            if isinstance(content_dict, dict):\n",
    "                for key, paragraph in content_dict.items():\n",
    "                    if paragraph.strip():  # Ignore empty paragraphs\n",
    "                        documents_to_upload.append({\n",
    "                            \"base_id\": base_id,  # Store base ID instead of doc_id\n",
    "                            \"document_name\": document_name,\n",
    "                            \"document_type\": document_type,\n",
    "                            \"document_link\": document_link,\n",
    "                            \"issuer\": issuer,\n",
    "                            \"resource_name\": resource_name,\n",
    "                            \"content\": paragraph.strip()\n",
    "                        })\n",
    "\n",
    "    return documents_to_upload\n",
    "\n",
    "# Automatically load all JSON files in the 'revlon/' folder\n",
    "json_directory = \"revlon\"  # Folder containing JSON files\n",
    "parsed_documents = load_documents_from_json(json_directory)\n",
    "\n",
    "# Output result\n",
    "if len(parsed_documents) == 0:\n",
    "    print(\"No documents found.\")\n",
    "else:\n",
    "    for doc in parsed_documents:\n",
    "        print(json.dumps(doc, indent=2))  # Pretty print the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_bulk_documents(parsed_documents)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
